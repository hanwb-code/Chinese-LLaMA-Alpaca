{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quRXOPaZwmwz",
    "outputId": "6d9febc4-b0d2-41ee-ce66-9284758e928e"
   },
   "outputs": [],
   "source": [
    "#下载原LLaMa模型\n",
    "!git clone https://github.com/Elyah2035/llama-dl/blob/main/llama.sh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dulrlPMexFNN",
    "outputId": "b9168c88-e72e-4f18-9450-d28832f7fe58"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.28.1\n",
    "!python -m transformers.models.llama.convert_llama_weights_to_hf \\\n",
    "    --input_dir /home/llama/llama-dl/llama_7b \\\n",
    "    --model_size 7B \\\n",
    "    --output_dir /home/llama/llama-dl/llama_7b_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cd Chinese-LLaMA-Alpaca && python scripts/merge_llama_with_chinese_lora.py \\\n",
    "    --base_model /home/llama/llama-dl/llama_7b_hf \\\n",
    "    --lora_model /home/llama/chinese-llama-plus-lora-7b,/home/llama/chinese-alpaca-plus-lora-7b \\\n",
    "    --output_type huggingface \\\n",
    "    --output_dir /home/llama/merge_alpaca_plus_lora_7b_pt\n",
    "\n",
    "#验证SHA256值  Chinese-Alpaca-Plus-7B 8b8f6551d0d83f93e378622b9f8dad0bec189da6c29d8a78de493e6aee9bd35f\n",
    "!sha256sum /home/merge_alpaca_plus_lora_7b_pt/consolidated.00.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir Chinese-LLaMA-Alpaca/pt_data\n",
    "!cp Chinese-LLaMA-Alpaca/data/pt_sample_data.txt Chinese-LLaMA-Alpaca/pt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IhrAVNUKSw9_",
    "outputId": "4f240f2b-2396-405b-9a94-84446db8d3e6"
   },
   "outputs": [],
   "source": [
    "!cd Chinese-LLaMA-Alpaca/scripts && torchrun --nnodes 1 --nproc_per_node 1 run_clm_pt_with_peft.py \\\n",
    "    --deepspeed ds_zero2_no_offload.json \\\n",
    "    --model_name_or_path decapoda-research/llama-7b-hf \\\n",
    "    --tokenizer_name_or_path ziqingyang/chinese-llama-lora-7b \\\n",
    "    --dataset_dir /content/Chinese-LLaMA-Alpaca/pt_data \\\n",
    "    --data_cache_dir data_cache \\\n",
    "    --validation_split_percentage 0.001 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --do_train \\\n",
    "    --fp16 \\\n",
    "    --seed $RANDOM \\\n",
    "    --max_steps 100 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --save_steps 50 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --preprocessing_num_workers 8 \\\n",
    "    --block_size 512 \\\n",
    "    --output_dir /content/output_model \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 32\\\n",
    "    --trainable q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj \\\n",
    "    --modules_to_save embed_tokens,lm_head \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype float16 \\\n",
    "    --gradient_checkpointing \\\n",
    "    --ddp_find_unused_parameters False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#启动模型demo\n",
    "!python scripts/inference/inference_hf.py --base_model /home/llama/merge_alpaca_plus_lora_7b_hf --with_prompt --interactive --gpus 0\n",
    "\n",
    "#启动web demo\n",
    "!python scripts/inference/gradio_demo.py --base_model /home/llama/merge_alpaca_plus_lora_7b_hf --gpus 0"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
