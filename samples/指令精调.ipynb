{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#合并 chinese-alpaca-plus-lora-7b\n",
    "!cd Chinese-LLaMA-Alpaca && python scripts/merge_llama_with_chinese_lora.py \\\n",
    "    --base_model /data/llama/llama_7b_hf \\\n",
    "    --lora_model /data/llama/chinese-llama-plus-lora-7b,/data/llama/chinese-alpaca-plus-lora-7b \\\n",
    "    --output_type huggingface \\\n",
    "    --output_dir /data/llama/merge_chinese_alpaca_plus_lora_7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备精调数据\n",
    "!mkdir Chinese-LLaMA-Alpaca/sft_data\n",
    "!cp Chinese-LLaMA-Alpaca/data/alpaca_data_zh_51k.json Chinese-LLaMA-Alpaca/sft_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IhrAVNUKSw9_",
    "outputId": "4f240f2b-2396-405b-9a94-84446db8d3e6"
   },
   "outputs": [],
   "source": [
    "#使用合并后的模型作为基模型训练\n",
    "#model_name_or_path         合并后的完整模型\n",
    "#tokenizer_name_or_path     扩充后的词表路径,这里使用合并后模型的词表也可以\n",
    "#peft_path                  训练好未合并的权重模型,比如chinese-llama-plus-lora-7b,chinese-alpaca-plus-lora-7b或者自己经过预训练得到的权重模型\n",
    "\n",
    "!cd Chinese-LLaMA-Alpaca/scripts/training && torchrun --nnodes 1 --nproc_per_node 1 run_clm_sft_with_peft.py \\\n",
    "    --deepspeed ds_zero2_no_offload.json \\\n",
    "    --model_name_or_path /data/llama/merge_nst_pt_chinese_alpaca_plus_lora_7b \\\n",
    "    --tokenizer_name_or_path /data/llama/merge_nst_pt_chinese_alpaca_plus_lora_7b \\\n",
    "    --dataset_dir /data/wenbo/Chinese-LLaMA-Alpaca/sft_data \\\n",
    "    --validation_split_percentage 0.001 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --do_train \\\n",
    "    --fp16 \\\n",
    "    --seed $RANDOM \\\n",
    "    --max_steps 100 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --weight_decay 0 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --save_steps 50 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --preprocessing_num_workers 8 \\\n",
    "    --max_seq_length 512 \\\n",
    "    --output_dir /data/wenbo/Chinese-LLaMA-Alpaca/output/sft_model \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --torch_dtype float16 \\\n",
    "    --peft_path /data/wenbo/Chinese-LLaMA-Alpaca/output/pt_model/pt_lora_model \\\n",
    "    --gradient_checkpointing \\\n",
    "    --ddp_find_unused_parameters False\n",
    "\n",
    "#将训练好的权重与基模型合并(这里的基础模型指的是merge_chinese_alpaca_plus_lora_7b合并后的模型)\n",
    "!cd Chinese-LLaMA-Alpaca && python scripts/merge_llama_with_chinese_lora.py \\\n",
    "    --base_model /data/llama/merge_chinese_alpaca_plus_lora_7b \\\n",
    "    --lora_model /data/wenbo/Chinese-LLaMA-Alpaca/output/sft_model/sft_lora_model \\\n",
    "    --output_type huggingface \\\n",
    "    --output_dir /data/llama/nst-sft-chinese-alpaca-plus-lora-7b\n",
    "\n",
    "#启动原训练合并后的hf模型演示\n",
    "!python scripts/inference/inference_hf.py --base_model /data/llama/nst-sft-chinese-alpaca-plus-lora-7b --with_prompt --interactive --gpus 0"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
